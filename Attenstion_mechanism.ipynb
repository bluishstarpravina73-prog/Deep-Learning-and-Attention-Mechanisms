{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgLiao2wM7FA",
        "outputId": "e61accbf-bb7f-4e5d-e774-9c96926d72e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Transformer...\n",
            "Epoch 1, Loss: 0.1096\n",
            "Epoch 2, Loss: 0.0195\n",
            "Epoch 3, Loss: 0.0138\n",
            "Epoch 4, Loss: 0.0125\n",
            "Epoch 5, Loss: 0.0099\n",
            "Epoch 6, Loss: 0.0100\n",
            "Epoch 7, Loss: 0.0090\n",
            "Epoch 8, Loss: 0.0079\n",
            "Epoch 9, Loss: 0.0065\n",
            "Epoch 10, Loss: 0.0068\n",
            "Epoch 11, Loss: 0.0077\n",
            "Epoch 12, Loss: 0.0072\n",
            "Epoch 13, Loss: 0.0090\n",
            "Epoch 14, Loss: 0.0069\n",
            "Epoch 15, Loss: 0.0071\n",
            "Epoch 16, Loss: 0.0085\n",
            "Epoch 17, Loss: 0.0078\n",
            "Epoch 18, Loss: 0.0083\n",
            "Epoch 19, Loss: 0.0067\n",
            "Epoch 20, Loss: 0.0061\n",
            "\n",
            "Transformer Performance\n",
            "MAE: 0.09234586\n",
            "RMSE: 0.11647644\n",
            "MAPE: 7.046276\n",
            "\n",
            "Training LSTM...\n",
            "Epoch 1, Loss: 0.3347\n",
            "Epoch 2, Loss: 0.0498\n",
            "Epoch 3, Loss: 0.0195\n",
            "Epoch 4, Loss: 0.0083\n",
            "Epoch 5, Loss: 0.0067\n",
            "Epoch 6, Loss: 0.0062\n",
            "Epoch 7, Loss: 0.0060\n",
            "Epoch 8, Loss: 0.0056\n",
            "Epoch 9, Loss: 0.0055\n",
            "Epoch 10, Loss: 0.0054\n",
            "Epoch 11, Loss: 0.0051\n",
            "Epoch 12, Loss: 0.0051\n",
            "Epoch 13, Loss: 0.0052\n",
            "Epoch 14, Loss: 0.0052\n",
            "Epoch 15, Loss: 0.0048\n",
            "Epoch 16, Loss: 0.0049\n",
            "Epoch 17, Loss: 0.0052\n",
            "Epoch 18, Loss: 0.0046\n",
            "Epoch 19, Loss: 0.0048\n",
            "Epoch 20, Loss: 0.0046\n",
            "\n",
            "LSTM Performance\n",
            "MAE: 0.089612305\n",
            "RMSE: 0.112681\n",
            "MAPE: 7.276498\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Advanced Time Series Forecasting\n",
        "# Transformer vs LSTM Baseline\n",
        "# ==============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==============================\n",
        "# 1. DATA GENERATION\n",
        "# ==============================\n",
        "\n",
        "def generate_time_series(n_steps=1200):\n",
        "    np.random.seed(42)\n",
        "    time = np.arange(n_steps)\n",
        "\n",
        "    trend = 0.05 * time\n",
        "    season1 = 10 * np.sin(2 * np.pi * time / 50)\n",
        "    season2 = 5 * np.cos(2 * np.pi * time / 100)\n",
        "\n",
        "    feature1 = trend + season1 + np.random.normal(0, 1, n_steps)\n",
        "    feature2 = trend * 0.5 + season2 + np.random.normal(0, 1, n_steps)\n",
        "    feature3 = season1 * 0.3 + np.random.normal(0, 0.5, n_steps)\n",
        "    feature4 = season2 * 0.7 + np.random.normal(0, 0.5, n_steps)\n",
        "    feature5 = trend * 0.2 + np.random.normal(0, 1, n_steps)\n",
        "\n",
        "    data = np.vstack([feature1, feature2, feature3, feature4, feature5]).T\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def create_sequences(data, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. TRANSFORMER MODEL\n",
        "# ==============================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_ff, d_model)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, attn_weights = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_dim,\n",
        "                 d_model=64,\n",
        "                 n_heads=4,\n",
        "                 num_layers=2,\n",
        "                 dim_ff=128):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(d_model, n_heads, dim_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_enc(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, attn_weights = layer(x)\n",
        "\n",
        "        out = self.output(x[:, -1, :])\n",
        "        return out, attn_weights\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 3. LSTM BASELINE\n",
        "# ==============================\n",
        "\n",
        "class LSTMBaseline(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        return self.fc(hidden[-1])\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 4. TRAINING FUNCTION\n",
        "# ==============================\n",
        "\n",
        "def train_model(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if isinstance(model, TimeSeriesTransformer):\n",
        "            outputs, _ = model(X_batch)\n",
        "        else:\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "        loss = criterion(outputs.squeeze(), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 5. EVALUATION\n",
        "# ==============================\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "\n",
        "            if isinstance(model, TimeSeriesTransformer):\n",
        "                outputs, attn = model(X_batch)\n",
        "            else:\n",
        "                outputs = model(X_batch)\n",
        "\n",
        "            y_true.extend(y_batch.numpy())\n",
        "            y_pred.extend(outputs.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    return mae, rmse, mape\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 6. MAIN EXECUTION\n",
        "# ==============================\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Generate data\n",
        "    df = generate_time_series()\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # Create sequences\n",
        "    X, y = create_sequences(data_scaled, seq_length=30)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(X_train, y_train),\n",
        "        batch_size=32,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(X_test, y_test),\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # ==========================\n",
        "    # Transformer Training\n",
        "    # ==========================\n",
        "\n",
        "    transformer = TimeSeriesTransformer(input_dim=5).to(device)\n",
        "    optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(\"Training Transformer...\")\n",
        "\n",
        "    for epoch in range(20):\n",
        "        loss = train_model(transformer, train_loader,\n",
        "                           optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    mae, rmse, mape = evaluate(transformer, test_loader, device)\n",
        "\n",
        "    print(\"\\nTransformer Performance\")\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print(\"MAPE:\", mape)\n",
        "\n",
        "    # ==========================\n",
        "    # LSTM Training\n",
        "    # ==========================\n",
        "\n",
        "    lstm = LSTMBaseline(input_dim=5).to(device)\n",
        "    optimizer_lstm = optim.Adam(lstm.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"\\nTraining LSTM...\")\n",
        "\n",
        "    for epoch in range(20):\n",
        "        loss = train_model(lstm, train_loader,\n",
        "                           optimizer_lstm, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    mae_lstm, rmse_lstm, mape_lstm = evaluate(\n",
        "        lstm, test_loader, device)\n",
        "\n",
        "    print(\"\\nLSTM Performance\")\n",
        "    print(\"MAE:\", mae_lstm)\n",
        "    print(\"RMSE:\", rmse_lstm)\n",
        "    print(\"MAPE:\", mape_lstm)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}